# SkewBalancer

**SkewBalancer** is a plug-and-play PySpark utility for **automatic skew detection and mitigation** using **value-based salting**, with built-in visual diagnostics, key detection, and optimized repartitioning.

> ðŸŽ¯ Built for real-world Spark pipelines â€” no manual tuning, no black magic.  
> ðŸ§  Works out of the box for both junior engineers and Spark veterans.

---

## ðŸš€ Features

- ðŸ” Automatically detects skewed numeric columns via IQR-based heuristics
- ðŸ§  Detects best column for `groupBy()` based on low cardinality
- ðŸ” Detects primary, composite, or surrogate keys â€” even UUIDs or alphanumerics
- ðŸŒ¶ï¸ Applies salting to dense or extreme ranges using percentile buckets
- âš™ï¸ Repartitions based on salted keys for optimal parallelism
- ðŸ“Š Generates:
  - Z-Score bar charts
  - Box plots
  - Histograms (before/after salting)
- ðŸ§¾ Logs full Spark `.explain()` plans (original vs salted)

---

## ðŸ“¦ Installation

```bash
git clone https://github.com/yourusername/SkewBalancer.git
cd SkewBalancer
pip install -e .
```

---

## ðŸ§ª Minimal Usage

```python
from skewbalancer import auto_balance_skew
from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local[*]").appName("SkewBalancer").getOrCreate()
df = spark.read.csv("your_file.csv", header=True, inferSchema=True)

df_balanced = auto_balance_skew(df, output_dir="outputs", partitions=10, verbose=True)
```

> âœ… Automatically selects:
> - Most skewed numeric column
> - Best groupBy key (low-cardinality)
> âœ… Salts dense zones
> âœ… Repartitions on salted keys
> âœ… Logs and saves 3 types of plots

---

## ðŸ§  How It Works

### ðŸ”¬ Skew Detection (IQR Logic)
```
1. For each numeric column:
2.     Compute Q1, Q2, Q3 percentiles
3.     Calculate skew = abs((Q3 - Q2) - (Q2 - Q1)) / IQR
4. Select the column with the highest skew > threshold
```

### ðŸ§‚ Smart Salting
```
1. Compute N percentiles to divide column into buckets
2. For each row:
    - Assign salt ID based on range bucket
    - Concatenate salt ID with original value
3. Create new salted_key column
```

### ðŸ” Key Detection
```
1. For each column:
    - Compute % of distinct values and nulls
    - Score = distinct_ratio - null_ratio
2. If score > 0.99 â†’ primary key
3. Else â†’ try composite combinations
4. Else â†’ check UUIDs or surrogates by length + uniqueness
```

### ðŸ“ Schema Inference
```
1. For all string columns:
    - Sample values
    - Infer if int, float, or string
2. Apply nullable=False if column is part of detected key
```

### ðŸ“‰ Partition Rebalancing
```
1. Repartition by salted key using df.repartition(N, salted_key)
2. Ensures Spark executors share workload evenly
```

---

## ðŸ§° Methods Overview

| Method | Description |
|--------|-------------|
| `run_full_balance_pipeline()` | Executes analyze â†’ salt â†’ repartition |
| `analyze_distribution()` | Computes p05/p95/mean/std |
| `apply_smart_salting()` | Buckets values and adds salt column |
| `repartition_on_salt()` | Rebalances DataFrame on salted keys |
| `infer_numeric_columns()` | Returns list of numeric columns |
| `detectKey()` | Detects primary/composite/surrogate keys |
| `schemaVisor()` | Infers Spark schema with nullable logic |
| `detect_skewed_column()` | Returns name of most skewed column |
| `detect_low_cardinality_categorical()` | Detects low-card string column for groupBy |
| `log_explain()` | Logs `.explain()` plan to file |
| `show_partition_sizes()` | Shows # of records per partition |
| `timeit()` | Measures execution time of any function |

---

## ðŸ“‚ Directory Structure

```
SkewBalancer/
â”œâ”€â”€ outputs/                      â† all logs and plots
â”œâ”€â”€ skewbalancer/
â”‚   â”œâ”€â”€ __init__.py              â† exposes `auto_balance_skew`
â”‚   â””â”€â”€ value_skew_balancer.py   â† core logic + class
â”œâ”€â”€ notebook_test.ipynb          â† usage example
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## âš¡ Real-World Use Cases

- Heavy `groupBy().agg()` operations in PySpark
- Customer churn segmentation
- Fraud detection or rare-event tracking
- Resource imbalance in Spark jobs
- Autotuning for unsupervised ETL jobs

---

## ðŸ”® Future Roadmap

- [ ] Auto-plot Spark executor metrics (stage-level metrics)
- [ ] Detect skew in categorical fields
- [ ] Support for Spark Structured Streaming
- [ ] Rebalance inside Delta Live Tables (DLT)
- [ ] Publish to PyPI and Databricks Marketplace

---

## ðŸ§  For Morons â€” Visual Process Breakdown

```
[Spark DataFrame]
       â†“
[Detect Most Skewed Numeric Column] â†â”€â”€ infer_numeric_columns() + skew logic
       â†“
[Detect Best GroupBy Key] â†â”€â”€ detect_low_cardinality_categorical()
       â†“
[Analyze Distribution] â†â”€â”€ analyze_distribution()
       â†“
[Apply Smart Salting] â†â”€â”€ apply_smart_salting()
       â†“
[Repartition on Salted Key] â†â”€â”€ repartition_on_salt()
       â†“
[Balanced DataFrame + Diagnostic Visuals]
```

---

## ðŸ‘¨â€ðŸ’» Author

**Omar Attia**  
Mid-level Data Engineer @ IBM  
ðŸ”— [github.com/OmarAttia95](https://github.com/OmarAttia95)

> This library was crafted with real-world Spark pain in mind â€” enjoy the speed and sanity.

---

## ðŸªª License

[MIT License](LICENSE)
